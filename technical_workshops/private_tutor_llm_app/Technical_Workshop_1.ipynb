{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Um9u30uXxph"
      },
      "outputs": [],
      "source": [
        "# Install OpenAI SDK\n",
        "!pip install openai\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "# configure your own openai api-key and add it to google collab secrets\n",
        "openai_api_key = userdata.get('openai-key')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What is an LLM?\n",
        "\n",
        "A **Large Language Model (LLM)** is a very large neural network trained to predict the next word in a sentence.  \n",
        "It read **billions of words** and can now:\n",
        "\n",
        "- Complete your sentences\n",
        "- Write essays or tweets\n",
        "- Answer questions\n",
        "- Generate code\n",
        "\n",
        "### ğŸ§’ Explain Like I'm 5:\n",
        "> Imagine a robot that read all the books in the world. When you ask it something, it uses everything it read to guess the best words to say next.\n",
        "\n",
        "# ğŸ’¬ Multi-Turn Chat with Roles Example\n",
        "\n",
        "This example demonstrates how you can maintain a **conversation** with the LLM using different `role` messages.\n",
        "\n",
        "### ğŸ§  Roles Recap:\n",
        "- `system`: Sets the overall behavior or tone of the assistant.\n",
        "- `user`: User input prompts.\n",
        "- `assistant`: Previous LLM responses (helps maintain context).\n",
        "\n",
        "---\n",
        "# ğŸ§® Tokens, Context\n",
        "\n",
        "### ğŸ”¹ What is a Token?\n",
        "- A token is like a chunk of text (e.g., a word or sub-word).\n",
        "- \"hackathon is cool!\" â†’ 4 tokens\n",
        "\n",
        "###ğŸ”¹ Contextâ€¯Window\n",
        "- LLMs remember a limited number of tokens per conversation:\n",
        "\n",
        "- GPTâ€‘3.5â€‘turbo: 4â€¯096â€‘token output cap (â‰ˆâ€¯shortâ€‘term memory).\n",
        "- GPTâ€‘4oâ€‘mini: up to 128â€¯000â€¯tokens in one promptâ€¯+â€¯response.\n",
        "\n",
        "When that limit is reached, the API simply drops the oldest messages, so early context vanishes.\n",
        "OpenAI Platform\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dWx9a_frYmDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "#\n",
        "messages = [\n",
        "    # â–¸ SYSTEM role: highâ€‘level behavior instructions. Parsed first.\n",
        "     {\"role\": \"system\",\n",
        "     \"content\": \"You are a patient science tutor who adapts explanations to a 5â€‘yearâ€‘old.\"},\n",
        "\n",
        "    # â–¸ USER role: the endâ€‘userâ€™s request or followâ€‘up.\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Explain black holes like I'm 5\"}\n",
        "]\n",
        "\n",
        "# 4) Call the new endpoint namespace openai.chat.completions.create\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# 5) Extract the answer and token usage (handy for cost tracking)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"Total tokens (prompt + completion):\", response.usage.total_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "55Z8S_PwX7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cost** calculation\n",
        "#### gpt-4o-mini Pricing per **1M** tokens\n",
        "                   \n",
        "| TokenÂ type | Priceâ€¯(USD) |\n",
        "| ---------- | ----------- |\n",
        "| **Input**  | **\\$0.15**  |\n",
        "| **Output** | **\\$0.60**  |\n"
      ],
      "metadata": {
        "id": "FtATjs7hCD3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mini_response_cost(response):\n",
        "    \"\"\"\n",
        "    Return the USD cost for a single GPTâ€‘4o mini call.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    response : openai.openai_object.OpenAIObject\n",
        "        The object returned by openai.ChatCompletion.create()\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Total cost in USD.\n",
        "    \"\"\"\n",
        "    PRICE_INPUT_PM  = 0.15\n",
        "    PRICE_OUTPUT_PM = 0.60\n",
        "    prompt_tokens     = response.usage.prompt_tokens # input tokens\n",
        "    completion_tokens = response.usage.completion_tokens # response tokens\n",
        "\n",
        "    # Compute costs\n",
        "    cost_input  = (prompt_tokens     * PRICE_INPUT_PM)  / 1_000_000\n",
        "    cost_output = (completion_tokens * PRICE_OUTPUT_PM) / 1_000_000\n",
        "\n",
        "    return cost_input + cost_output\n",
        "\n",
        "print(\"Total tokens (prompt + completion):\", response.usage.total_tokens)\n",
        "print(f\"Total cost: ${calculate_mini_response_cost(response=response):,.6f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GOECFpV-DNpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ğŸ—£ï¸ Example Conversation Flow:\n",
        "- The system tells the model to behave like a **funny tour guide **.\n",
        "- The user asks about the Eiffel Tower.\n",
        "- The assistant replies sarcastically .\n",
        "- The user follows up, and the LLM responds again using the context.\n",
        "\n",
        "### ğŸ”¹ Message Roles\n",
        "| Role        | Use Case                                 |\n",
        "|-------------|-------------------------------------------|\n",
        "| `system`    | Sets behavior and tone                   |\n",
        "| `user`      | Your input to the model                  |\n",
        "| `assistant` | Modelâ€™s reply (or past replies)          |\n",
        "| `tool`      | Where the model calls a tool/function    |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0wdFWCzSDbuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a funny tour guide.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about the Eiffel Tower.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Eiffel Tower is like a giant iron ice cream cone! Want to know how tall it is?\"},\n",
        "    {\"role\": \"user\", \"content\": \"where is this giant iron ice cream cone\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "2rt_gWxhX7pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Letâ€™s build an LLM **â€œPrivateâ€¯Tutorâ€** app  \n",
        "### A chatâ€‘based tutor that adapts explanations, examples, and practice questions to:\n",
        "\n",
        "* **Middleâ€¯School**\n",
        "  * Concrete, everyday examples  \n",
        "  * Visual aids and short explanations  \n",
        "\n",
        "* **Highâ€¯School**\n",
        "  * Explain scientific concepts\n",
        "  * Answers SAT style questions\n",
        "\n",
        "* **University**\n",
        "  * Deepâ€‘dive theory with detailed explaination   \n",
        "  * Researchâ€‘style problems and stepâ€‘byâ€‘step proofs  \n",
        "\n",
        "## let's explore prompting techniques\n",
        "  * zero-shot , one-shot , few-shot  \n",
        "  * chain of thought"
      ],
      "metadata": {
        "id": "jCq-wHs4JusG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# System prompts for each education level\n",
        "# ----------------------------------------\n",
        "system = {\n",
        "    # zero shot prompting\n",
        "    \"middle\": (\n",
        "        \"\"\"\n",
        "        You are a **middleâ€‘school** science tutor.\n",
        "        Use simple words, fun analogies, and short sentences.\"\"\"\n",
        "    ),\n",
        "\n",
        "    # one shot prompting\n",
        "    \"high\": (\n",
        "        \"\"\"\n",
        "        You are a **highâ€‘school** physics tutor.\n",
        "        Give clear explanations, answered SAT exams examples\n",
        "         one-shot-example:\n",
        "    user : What are electrons?\n",
        "    assistant : Electrons are tiny, negativelyâ€‘charged subâ€‘atomic particles\n",
        "    that orbit the nucleus of an atom.  Theyâ€™re about 1â„2000 the mass of a\n",
        "    proton, and their arrangement in â€œshellsâ€ determines an elementâ€™s\n",
        "    chemical behavior.  SAT tip: think of electrons as the mobile charge\n",
        "    carriers in a circuitâ€”when a question mentions electric current, itâ€™s\n",
        "    the flow of electrons that earns you the point.\n",
        "    \"\"\"\n",
        "    ),\n",
        "\n",
        "     # chain of thought prompting\n",
        "    \"university\": (\n",
        "        \"\"\"\n",
        "        You are a **universityâ€level astrophysics lecturer**.\n",
        "\n",
        "    **Chainâ€‘ofâ€‘Thought Requirement**\n",
        "    â€¢ Think stepâ€‘byâ€‘step, stating assumptions and intermediate deductions.\n",
        "    â€¢ Write relevant equations and define each variable.\n",
        "    â€¢ Do **not** hide reasoning; show the logical flow that leads to the answer.\n",
        "    â€¢ If the question is ambiguous, ask one clarifying question before proceeding.\n",
        "\n",
        "        \"\"\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "user_query = \"Explain black holes\"\n"
      ],
      "metadata": {
        "id": "xfgakvKVZMOh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Middle School Level"
      ],
      "metadata": {
        "id": "7xOKQvq4Eo_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages_middle = [\n",
        "    {\"role\": \"system\", \"content\": system[\"middle\"]},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "]\n",
        "response_middle = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages_middle\n",
        ")\n",
        "print(\"\\n--- Middle School Level ---\")\n",
        "print(response_middle.choices[0].message.content)"
      ],
      "metadata": {
        "id": "HTF8BZzvEocI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High School Level"
      ],
      "metadata": {
        "id": "vIodB_tZE4q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages_high = [\n",
        "    {\"role\": \"system\", \"content\": system[\"high\"]},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "]\n",
        "response_high = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages_high\n",
        ")\n",
        "print(\"\\n--- High School Level ---\")\n",
        "print(response_high.choices[0].message.content)"
      ],
      "metadata": {
        "id": "w8m4hClNE6lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# University Level"
      ],
      "metadata": {
        "id": "ZtF0yzjsFc5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages_high = [\n",
        "    {\"role\": \"system\", \"content\": system[\"university\"]},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "]\n",
        "response_high = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages_high\n",
        ")\n",
        "print(\"\\n--- University Level ---\")\n",
        "print(response_high.choices[0].message.content)"
      ],
      "metadata": {
        "id": "1IJGcdQGFhqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ” Retrievalâ€‘Augmented GenerationÂ (RAG)\n",
        "\n",
        "RAG **retrieves** the most relevant passages from a knowledge base and **augments** the LLMâ€™s prompt with those passages before it generates an answer.\n",
        "\n",
        "### Why RAG matters for the Privateâ€¯Tutor\n",
        "\n",
        "- **Reinforces context.** Supplies upâ€‘toâ€‘date facts and equations that may be missing from the modelâ€™s parameters.  \n",
        "- **Reduces hallucinations.** Grounding the answer in retrieved text keeps explanations accurate and complete.  \n",
        "- **Delivers key formulas on demand.** Ensures students see the exact equations they need, not a guess or omission.  \n"
      ],
      "metadata": {
        "id": "5pNIJqO_T9wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Tiny Knowledgeâ€‘Base + RAG Helper\n",
        "\n",
        "# can be replaced by a vector search database containing university books and formulas .\n",
        "# You can use pinecone , chromadb , weviate , any vector database provider\n",
        "KB = [\n",
        "    \"Newtonâ€™s law of universal gravitation:  F = GÂ·mâ‚mâ‚‚ / rÂ²  where  G = 6.67430â€¯Ã—â€¯10â»Â¹Â¹â€¯NÂ·mÂ²Â·kgâ»Â².\",\n",
        "\n",
        "    \"Einsteinâ€™s massâ€“energy equivalence:  E = mÂ·cÂ²  with  c = 2.99792458â€¯Ã—â€¯10â¸â€¯mâ€¯sâ»Â¹.\",\n",
        "\n",
        "    \"Faradayâ€“Lenz law of induction:  âˆ‡â€¯Ã—â€¯E = âˆ’âˆ‚B/âˆ‚t,  relating a changing magnetic field B to an induced electric field E.\",\n",
        "\n",
        "    \"Quantum mechanics:  iÄ§Â·âˆ‚Ïˆ/âˆ‚t = Ä¤Ïˆ,  where Ä§ = 1.054â€¯Ã—â€¯10â»Â³â´â€¯JÂ·s and Ä¤ is the Hamiltonian operator.\",\n",
        "\n",
        "    \"Chemical thermodynamics:  Î”G = Î”H âˆ’ TÎ”S.  A reaction is spontaneous at constant T and P when Î”G < 0.\",\n",
        "\n",
        "    \"Ideal gas law:  PÂ·V = nÂ·RÂ·T,  with R = 8.314â€¯JÂ·molâ»Â¹Â·Kâ»Â¹.\",\n",
        "\n",
        "    \"Fluid dynamics (incompressible):  Ï(âˆ‚v/âˆ‚t + vÂ·âˆ‡v) = âˆ’âˆ‡p + Î¼âˆ‡Â²v,  linking momentum, pressure p and viscosity Î¼.\",\n",
        "\n",
        "    \"Signal processing:  F(Ï‰) = âˆ«_{âˆ’âˆ}^{âˆ} f(t)Â·e^{âˆ’iÏ‰t} dt  and  f(t) = (1/2Ï€)âˆ«_{âˆ’âˆ}^{âˆ} F(Ï‰)Â·e^{iÏ‰t} dÏ‰.\",\n",
        "\n",
        "    \"Blackâ€‘body radiation:  B(Î»,T) = (2hcÂ²/Î»âµ)Â·1/(e^{hc/(Î»kT)} âˆ’ 1),  with h = 6.626â€¯Ã—â€¯10â»Â³â´â€¯JÂ·s.\",\n",
        "\n",
        "    \"Bernoulliâ€™s equation for inviscid flow:  p + Â½ÏvÂ² + Ïgh = constant along a streamline.\"\n",
        "]\n",
        "\n",
        "\n",
        "import  numpy as np\n",
        "EMBED_MODEL = \"text-embedding-3-small\" # OPENAI embeddeing model\n",
        "\n",
        "kb_vecs = client.embeddings.create(\n",
        "    model=EMBED_MODEL,\n",
        "    input=KB\n",
        ")\n",
        "kb_vecs = np.array([d.embedding for d in kb_vecs.data])  # kb_vecs is our mini vector database\n",
        "\n",
        "# Simple cosineâ€‘similarity retriever\n",
        "# top_k = number of chunks retrieved\n",
        "def retrieve_university_books(query: str, top_k: int = 1) -> list[str]:\n",
        "    \"\"\"Return the topâ€‘k relevant chapters facts most relevant to `query`.\"\"\"\n",
        "    q_vec = np.array(\n",
        "        client.embeddings.create(model=EMBED_MODEL, input=[query]).data[0].embedding\n",
        "    )\n",
        "    sims = kb_vecs @ q_vec / (np.linalg.norm(kb_vecs, axis=1) * np.linalg.norm(q_vec))\n",
        "    top_ids = sims.argsort()[-top_k:][::-1]\n",
        "    return [KB[i] for i in top_ids]\n",
        "\n",
        "print(\"Top facts for:  What causes gravity?\\n\")\n",
        "for formula in retrieve_university_books(\"What causes gravity?\"):\n",
        "    print(\"related formula : \", formula)\n"
      ],
      "metadata": {
        "id": "iSMxf369ayQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "btJuL1d6X5Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reinforced by RAG\n",
        "def answer_university_student(query: str):\n",
        "    \"\"\"\n",
        "    Retrieve context from the KB and return a chainâ€‘ofâ€‘thought answer.\n",
        "    (k is fixed inside retrieve_university_books; caller doesnâ€™t need to pass it.)\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join(retrieve_university_books(query))\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system['university']},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Reference book:\\n{context}\"}, # add the retrieved context\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "print(answer_university_student(\"What causes gravity?\"))\n"
      ],
      "metadata": {
        "id": "VwPIPfV8ZL-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ› ï¸ Tool Calling Setup (Function Calling)\n",
        "\n",
        "LLMs like GPT-4o-mini can go beyond just generating text â€” they can **request actions** using tools you define.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¹ What is Tool Calling?\n",
        "Instead of responding with plain text, the LLM can say:\n",
        "> â€œI want to call a function named `XXXXX` with this input.â€\n",
        "\n",
        "Your app will:\n",
        "1. Detect this tool call.\n",
        "2. Run the real function (like a Python script or API call).\n",
        "3. Pass the result **back to the LLM** to continue the conversation.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”§ In This Example:\n",
        "We define a **language transcribtion** named `gpt-to-egytpian` with:\n",
        "- A `name`: what the LLM calls.\n",
        "- A `description`: tells the LLM what it does.\n",
        "- `parameters`: what input the tool needs (`query` in this case).\n",
        "\n",
        "This tool simulates an API that searches for trending topics.\n",
        "### ğŸ“¨ Tool Message Type\n",
        "When the LLM wants to invoke a tool, it responds with a special **`tool` message type** rather than plain text. This message includes the function name and a structured JSON input.  \n",
        "Your system detects this message and triggers the corresponding tool before resuming the LLM conversation with the result.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rso-BuUxZDzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# ğŸ› ï¸ Tool definition\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"gpt_to_egyptian\",\n",
        "            \"description\": \"Translate English text into Egyptianâ€‘Arabic \",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"text\": {\"type\": \"string\", \"description\": \"English text\"}\n",
        "                },\n",
        "                \"required\": [\"text\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def gpt_to_egyptian(text: str) -> str:\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Translate to friendly Egyptian Arabic in a sarcastic manner.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "    return r.choices[0].message.content.strip()\n",
        "\n",
        "queries = [\n",
        "    \"Please translate to Arabic: 'Good morning, everyone!'\",\n",
        "    \"What is the Pythagorean theorem?\",\n",
        "    \"Translate to Arabic: 'I love programming in Python.'\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(f\"\\nğŸŸ  USER QUERY: {q}\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are an assistant Tutor . \"\n",
        "                \"If the user asks to translate English into Arabic, \"\n",
        "                \"call the function `gpt_to_egyptian` with the text. \"\n",
        "                \"Otherwise, answer normally.\"\n",
        "            )\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": q}\n",
        "    ]\n",
        "\n",
        "    first = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        tools=tools\n",
        "    ).choices[0]\n",
        "\n",
        "    if first.finish_reason == \"tool_calls\":\n",
        "        call = first.message.tool_calls[0]\n",
        "        args = json.loads(call.function.arguments)\n",
        "        print(\"ğŸ”§ TOOL CALLED with args:\", args)\n",
        "\n",
        "        # Perform translation\n",
        "        arabic_text = gpt_to_egyptian(args[\"text\"])\n",
        "        print(\"ğŸŒ Translation returned:\", arabic_text)\n",
        "\n",
        "        # Send tool message back to GPT\n",
        "        tool_msg = {\n",
        "            \"role\": \"tool\",\n",
        "            \"tool_call_id\": call.id,\n",
        "            \"name\": \"gpt_to_egyptian\",\n",
        "            \"content\": arabic_text\n",
        "        }\n",
        "\n",
        "        final = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages + [first.message, tool_msg]\n",
        "        )\n",
        "        print(\"ğŸŸ¢ GPT FINAL ANSWER:\", final.choices[0].message.content)\n",
        "\n",
        "    else:\n",
        "        # No tool; GPT responds directly\n",
        "        print(\"ğŸŸ¢ GPT DIRECT ANSWER:\", first.message.content)"
      ],
      "metadata": {
        "id": "u8LAW5zgZDEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promptâ€¯Engineering  \n",
        "&nbsp;&nbsp;â†“  \n",
        "RAGâ€¯Contextâ€¯Retrieval  \n",
        "&nbsp;&nbsp;â†“  \n",
        "LLMâ€¯Passâ€¯#1â€¯â€”â€¯Draftâ€¯Englishâ€¯Answer  \n",
        "&nbsp;&nbsp;â†“  \n",
        "Toolâ€‘Callâ€¯Decision  \n",
        "&nbsp;&nbsp;â†“  \n",
        "&nbsp;&nbsp;â”œâ”€â€¯Arabicâ€¯requestedâ€¯â†’â€¯Translationâ€¯Toolâ€¯(`gpt_to_egyptian`)â€¯â†’â€¯Finalâ€¯Arabicâ€¯Answer  \n",
        "&nbsp;&nbsp;â””â”€â€¯Otherwiseâ€¯â†’â€¯Finalâ€¯Englishâ€¯Answer\n"
      ],
      "metadata": {
        "id": "Z0K1xgRiKgQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------  Full application layer ----------------\n",
        "def private_university_tutor(query: str) -> str:\n",
        "  #1) RAG ( context retrieval )\n",
        "    context = \"\\n\".join(retrieve_university_books(query))\n",
        "    print(f\"\\nğŸ”¹ QUERY: {query}\")\n",
        "    print(f\"ğŸ”¹ Retrieved context:\\n{context}\\n\")\n",
        "    #2) prompt engineering\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a **universityâ€level astrophysics lecturer \"\n",
        "                \"Use the provided reference to craft a concise, accurate answer. \"\n",
        "                \"Think stepâ€‘byâ€‘step, stating assumptions and intermediate deductions.\"\n",
        "                \"Write relevant equations and define each variable.\"\n",
        "                \"Do **not** hide reasoning; show the logical flow that leads to the answer\"\n",
        "                \"If the student explicitly requests Arabic output\"\n",
        "                \"(e.g. 'in Arabic', 'Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ', 'translate to Arabic'), \"\n",
        "                \"call the function `gpt_to_egyptian` with your English answer. \"\n",
        "                \"Otherwise, answer in English.\"\n",
        "            ),\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": f\"Reference book:\\n{context}\"},\n",
        "        {\"role\": \"user\",  \"content\": query},\n",
        "    ]\n",
        "\n",
        "    first = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "    ).choices[0]\n",
        "\n",
        "    print(\"ğŸ”¹ finish_reason:\", first.finish_reason)\n",
        "    print(\"ğŸ”¹ First assistant message:\\n\", first.message.content, \"\\n\")\n",
        "#3) tool calling\n",
        "    if first.finish_reason == \"tool_calls\":\n",
        "        call = first.message.tool_calls[0]\n",
        "        args = json.loads(call.function.arguments)\n",
        "        print(\"ğŸ”§ Tool call detected:\", call.function.name, \"with\", args)\n",
        "\n",
        "        arabic = gpt_to_egyptian(args[\"text\"])\n",
        "        print(\"ğŸŒ Local translation result:\\n\", arabic, \"\\n\")\n",
        "\n",
        "        print(\"ğŸŸ¢ Final assistant answer (Arabic):\\n\", arabic, \"\\n\")\n",
        "        return arabic\n",
        "\n",
        "    print(\"ğŸŸ¢ Final assistant answer (no tool needed):\\n\", first.message.content, \"\\n\")\n",
        "    return first.message.content\n",
        "\n",
        "\n",
        "private_university_tutor(\"What causes gravity?\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70 + \"\\n\")\n",
        "\n",
        "private_university_tutor(\"Explain Newton's law of gravity in Arabic, please.\")\n"
      ],
      "metadata": {
        "id": "ZrjZdt-sKgkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
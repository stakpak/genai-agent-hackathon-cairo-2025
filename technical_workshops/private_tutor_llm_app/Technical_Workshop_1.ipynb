{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Um9u30uXxph"
      },
      "outputs": [],
      "source": [
        "# Install OpenAI SDK\n",
        "!pip install openai\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "# configure your own openai api-key and add it to google collab secrets\n",
        "openai_api_key = userdata.get('openai-key')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What is an LLM?\n",
        "\n",
        "A **Large Language Model (LLM)** is a very large neural network trained to predict the next word in a sentence.  \n",
        "It read **billions of words** and can now:\n",
        "\n",
        "- Complete your sentences\n",
        "- Write essays or tweets\n",
        "- Answer questions\n",
        "- Generate code\n",
        "\n",
        "### üßí Explain Like I'm 5:\n",
        "> Imagine a robot that read all the books in the world. When you ask it something, it uses everything it read to guess the best words to say next.\n",
        "\n",
        "# üí¨ Multi-Turn Chat with Roles Example\n",
        "\n",
        "This example demonstrates how you can maintain a **conversation** with the LLM using different `role` messages.\n",
        "\n",
        "### üß† Roles Recap:\n",
        "- `system`: Sets the overall behavior or tone of the assistant.\n",
        "- `user`: User input prompts.\n",
        "- `assistant`: Previous LLM responses (helps maintain context).\n",
        "\n",
        "---\n",
        "# üßÆ Tokens, Context\n",
        "\n",
        "### üîπ What is a Token?\n",
        "- A token is like a chunk of text (e.g., a word or sub-word).\n",
        "- \"hackathon is cool!\" ‚Üí 4 tokens\n",
        "\n",
        "###üîπ Context‚ÄØWindow\n",
        "- LLMs remember a limited number of tokens per conversation:\n",
        "\n",
        "- GPT‚Äë3.5‚Äëturbo: 4‚ÄØ096‚Äëtoken output cap (‚âà‚ÄØshort‚Äëterm memory).\n",
        "- GPT‚Äë4o‚Äëmini: up to 128‚ÄØ000‚ÄØtokens in one prompt‚ÄØ+‚ÄØresponse.\n",
        "\n",
        "When that limit is reached, the API simply drops the oldest messages, so early context vanishes.\n",
        "OpenAI Platform\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dWx9a_frYmDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "#\n",
        "messages = [\n",
        "    # ‚ñ∏ SYSTEM role: high‚Äëlevel behavior instructions. Parsed first.\n",
        "     {\"role\": \"system\",\n",
        "     \"content\": \"You are a patient science tutor who adapts explanations to a 5‚Äëyear‚Äëold.\"},\n",
        "\n",
        "    # ‚ñ∏ USER role: the end‚Äëuser‚Äôs request or follow‚Äëup.\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Explain black holes like I'm 5\"}\n",
        "]\n",
        "\n",
        "# 4) Call the new endpoint namespace openai.chat.completions.create\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# 5) Extract the answer and token usage (handy for cost tracking)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"Total tokens (prompt + completion):\", response.usage.total_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "55Z8S_PwX7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cost** calculation\n",
        "#### gpt-4o-mini Pricing per **1M** tokens\n",
        "                   \n",
        "| Token¬†type | Price‚ÄØ(USD) |\n",
        "| ---------- | ----------- |\n",
        "| **Input**  | **\\$0.15**  |\n",
        "| **Output** | **\\$0.60**  |\n"
      ],
      "metadata": {
        "id": "FtATjs7hCD3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mini_response_cost(response):\n",
        "    \"\"\"\n",
        "    Return the USD cost for a single GPT‚Äë4o mini call.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    response : openai.openai_object.OpenAIObject\n",
        "        The object returned by openai.ChatCompletion.create()\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Total cost in USD.\n",
        "    \"\"\"\n",
        "    PRICE_INPUT_PM  = 0.15\n",
        "    PRICE_OUTPUT_PM = 0.60\n",
        "    prompt_tokens     = response.usage.prompt_tokens # input tokens\n",
        "    completion_tokens = response.usage.completion_tokens # response tokens\n",
        "\n",
        "    # Compute costs\n",
        "    cost_input  = (prompt_tokens     * PRICE_INPUT_PM)  / 1_000_000\n",
        "    cost_output = (completion_tokens * PRICE_OUTPUT_PM) / 1_000_000\n",
        "\n",
        "    return cost_input + cost_output\n",
        "\n",
        "print(\"Total tokens (prompt + completion):\", response.usage.total_tokens)\n",
        "print(f\"Total cost: ${calculate_mini_response_cost(response=response):,.6f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GOECFpV-DNpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üó£Ô∏è Example Conversation Flow:\n",
        "- The system tells the model to behave like a **funny tour guide **.\n",
        "- The user asks about the Eiffel Tower.\n",
        "- The assistant replies sarcastically .\n",
        "- The user follows up, and the LLM responds again using the context.\n",
        "\n",
        "### üîπ Message Roles\n",
        "| Role        | Use Case                                 |\n",
        "|-------------|-------------------------------------------|\n",
        "| `system`    | Sets behavior and tone                   |\n",
        "| `user`      | Your input to the model                  |\n",
        "| `assistant` | Model‚Äôs reply (or past replies)          |\n",
        "| `tool`      | Where the model calls a tool/function    |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0wdFWCzSDbuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a funny tour guide.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about the Eiffel Tower.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Eiffel Tower is like a giant iron ice cream cone! Want to know how tall it is?\"},\n",
        "    {\"role\": \"user\", \"content\": \"where is this giant iron ice cream cone\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "2rt_gWxhX7pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let‚Äôs build an LLM **‚ÄúPrivate‚ÄØTutor‚Äù** app  \n",
        "### A chat‚Äëbased tutor that adapts explanations, examples, and practice questions to:\n",
        "\n",
        "* **Middle‚ÄØSchool**\n",
        "  * Concrete, everyday examples  \n",
        "  * Visual aids and short explanations  \n",
        "\n",
        "* **High‚ÄØSchool**\n",
        "  * Explain scientific concepts\n",
        "  * Answers SAT style questions\n",
        "\n",
        "* **University**\n",
        "  * Deep‚Äëdive theory with detailed explaination   \n",
        "  * Research‚Äëstyle problems and step‚Äëby‚Äëstep proofs  \n",
        "\n",
        "## let's explore prompting techniques\n",
        "  * zero-shot , one-shot , few-shot  \n",
        "  * chain of thought"
      ],
      "metadata": {
        "id": "jCq-wHs4JusG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# System prompts for each education level\n",
        "# ----------------------------------------\n",
        "system = {\n",
        "    # zero shot prompting\n",
        "    \"middle\": (\n",
        "        \"\"\"\n",
        "        You are a **middle‚Äëschool** science tutor.\n",
        "        Use simple words, fun analogies, and short sentences.\"\"\"\n",
        "    ),\n",
        "\n",
        "    # one shot prompting\n",
        "    \"high\": (\n",
        "        \"\"\"\n",
        "        You are a **high‚Äëschool** physics tutor.\n",
        "        Give clear explanations, answered SAT exams examples\n",
        "         one-shot-example:\n",
        "    user : What are electrons?\n",
        "    assistant : Electrons are tiny, negatively‚Äëcharged sub‚Äëatomic particles\n",
        "    that orbit the nucleus of an atom.  They‚Äôre about 1‚ÅÑ2000 the mass of a\n",
        "    proton, and their arrangement in ‚Äúshells‚Äù determines an element‚Äôs\n",
        "    chemical behavior.  SAT tip: think of electrons as the mobile charge\n",
        "    carriers in a circuit‚Äîwhen a question mentions electric current, it‚Äôs\n",
        "    the flow of electrons that earns you the point.\n",
        "    \"\"\"\n",
        "    ),\n",
        "\n",
        "     # chain of thought prompting\n",
        "    \"university\": (\n",
        "        \"\"\"\n",
        "        You are a **university‚Äêlevel astrophysics lecturer**.\n",
        "\n",
        "    **Chain‚Äëof‚ÄëThought Requirement**\n",
        "    ‚Ä¢ Think step‚Äëby‚Äëstep, stating assumptions and intermediate deductions.\n",
        "    ‚Ä¢ Write relevant equations and define each variable.\n",
        "    ‚Ä¢ Do **not** hide reasoning; show the logical flow that leads to the answer.\n",
        "    ‚Ä¢ If the question is ambiguous, ask one clarifying question before proceeding.\n",
        "\n",
        "        \"\"\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "user_query = \"Explain black holes\"\n"
      ],
      "metadata": {
        "id": "xfgakvKVZMOh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Middle School Level"
      ],
      "metadata": {
        "id": "7xOKQvq4Eo_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages_middle = [\n",
        "    {\"role\": \"system\", \"content\": system[\"middle\"]},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "]\n",
        "response_middle = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages_middle\n",
        ")\n",
        "print(\"\\n--- Middle School Level ---\")\n",
        "print(response_middle.choices[0].message.content)"
      ],
      "metadata": {
        "id": "HTF8BZzvEocI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High School Level"
      ],
      "metadata": {
        "id": "vIodB_tZE4q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages_high = [\n",
        "    {\"role\": \"system\", \"content\": system[\"high\"]},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "]\n",
        "response_high = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages_high\n",
        ")\n",
        "print(\"\\n--- High School Level ---\")\n",
        "print(response_high.choices[0].message.content)"
      ],
      "metadata": {
        "id": "w8m4hClNE6lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# University Level"
      ],
      "metadata": {
        "id": "ZtF0yzjsFc5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages_high = [\n",
        "    {\"role\": \"system\", \"content\": system[\"university\"]},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "]\n",
        "response_high = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages_high\n",
        ")\n",
        "print(\"\\n--- University Level ---\")\n",
        "print(response_high.choices[0].message.content)"
      ],
      "metadata": {
        "id": "1IJGcdQGFhqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Retrieval‚ÄëAugmented Generation¬†(RAG)\n",
        "\n",
        "RAG **retrieves** the most relevant passages from a knowledge base and **augments** the LLM‚Äôs prompt with those passages before it generates an answer.\n",
        "\n",
        "### Why RAG matters for the Private‚ÄØTutor\n",
        "\n",
        "- **Reinforces context.** Supplies up‚Äëto‚Äëdate facts and equations that may be missing from the model‚Äôs parameters.  \n",
        "- **Reduces hallucinations.** Grounding the answer in retrieved text keeps explanations accurate and complete.  \n",
        "- **Delivers key formulas on demand.** Ensures students see the exact equations they need, not a guess or omission.  \n"
      ],
      "metadata": {
        "id": "5pNIJqO_T9wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Tiny Knowledge‚ÄëBase + RAG Helper\n",
        "\n",
        "# can be replaced by a vector search database containing university books and formulas .\n",
        "# You can use pinecone , chromadb , weviate , any vector database provider\n",
        "KB = [\n",
        "    \"Newton‚Äôs law of universal gravitation:  F = G¬∑m‚ÇÅm‚ÇÇ / r¬≤  where  G = 6.67430‚ÄØ√ó‚ÄØ10‚Åª¬π¬π‚ÄØN¬∑m¬≤¬∑kg‚Åª¬≤.\",\n",
        "\n",
        "    \"Einstein‚Äôs mass‚Äìenergy equivalence:  E = m¬∑c¬≤  with  c = 2.99792458‚ÄØ√ó‚ÄØ10‚Å∏‚ÄØm‚ÄØs‚Åª¬π.\",\n",
        "\n",
        "    \"Faraday‚ÄìLenz law of induction:  ‚àá‚ÄØ√ó‚ÄØE = ‚àí‚àÇB/‚àÇt,  relating a changing magnetic field B to an induced electric field E.\",\n",
        "\n",
        "    \"Quantum mechanics:  iƒß¬∑‚àÇœà/‚àÇt = ƒ§œà,  where ƒß = 1.054‚ÄØ√ó‚ÄØ10‚Åª¬≥‚Å¥‚ÄØJ¬∑s and ƒ§ is the Hamiltonian operator.\",\n",
        "\n",
        "    \"Chemical thermodynamics:  ŒîG = ŒîH ‚àí TŒîS.  A reaction is spontaneous at constant T and P when ŒîG < 0.\",\n",
        "\n",
        "    \"Ideal gas law:  P¬∑V = n¬∑R¬∑T,  with R = 8.314‚ÄØJ¬∑mol‚Åª¬π¬∑K‚Åª¬π.\",\n",
        "\n",
        "    \"Fluid dynamics (incompressible):  œÅ(‚àÇv/‚àÇt + v¬∑‚àáv) = ‚àí‚àáp + Œº‚àá¬≤v,  linking momentum, pressure p and viscosity Œº.\",\n",
        "\n",
        "    \"Signal processing:  F(œâ) = ‚à´_{‚àí‚àû}^{‚àû} f(t)¬∑e^{‚àíiœât} dt  and  f(t) = (1/2œÄ)‚à´_{‚àí‚àû}^{‚àû} F(œâ)¬∑e^{iœât} dœâ.\",\n",
        "\n",
        "    \"Black‚Äëbody radiation:  B(Œª,T) = (2hc¬≤/Œª‚Åµ)¬∑1/(e^{hc/(ŒªkT)} ‚àí 1),  with h = 6.626‚ÄØ√ó‚ÄØ10‚Åª¬≥‚Å¥‚ÄØJ¬∑s.\",\n",
        "\n",
        "    \"Bernoulli‚Äôs equation for inviscid flow:  p + ¬ΩœÅv¬≤ + œÅgh = constant along a streamline.\"\n",
        "]\n",
        "\n",
        "\n",
        "import  numpy as np\n",
        "EMBED_MODEL = \"text-embedding-3-small\" # OPENAI embeddeing model\n",
        "\n",
        "kb_vecs = client.embeddings.create(\n",
        "    model=EMBED_MODEL,\n",
        "    input=KB\n",
        ")\n",
        "kb_vecs = np.array([d.embedding for d in kb_vecs.data])  # kb_vecs is our mini vector database\n",
        "\n",
        "# Simple cosine‚Äësimilarity retriever\n",
        "# top_k = number of chunks retrieved\n",
        "def retrieve_university_books(query: str, top_k: int = 1) -> list[str]:\n",
        "    \"\"\"Return the top‚Äëk relevant chapters facts most relevant to `query`.\"\"\"\n",
        "    q_vec = np.array(\n",
        "        client.embeddings.create(model=EMBED_MODEL, input=[query]).data[0].embedding\n",
        "    )\n",
        "    sims = kb_vecs @ q_vec / (np.linalg.norm(kb_vecs, axis=1) * np.linalg.norm(q_vec))\n",
        "    top_ids = sims.argsort()[-top_k:][::-1]\n",
        "    return [KB[i] for i in top_ids]\n",
        "\n",
        "print(\"Top facts for:  What causes gravity?\\n\")\n",
        "for formula in retrieve_university_books(\"What causes gravity?\"):\n",
        "    print(\"related formula : \", formula)\n"
      ],
      "metadata": {
        "id": "iSMxf369ayQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "btJuL1d6X5Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reinforced by RAG\n",
        "def answer_university_student(query: str):\n",
        "    \"\"\"\n",
        "    Retrieve context from the KB and return a chain‚Äëof‚Äëthought answer.\n",
        "    (k is fixed inside retrieve_university_books; caller doesn‚Äôt need to pass it.)\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join(retrieve_university_books(query))\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system['university']},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Reference book:\\n{context}\"}, # add the retrieved context\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "print(answer_university_student(\"What causes gravity?\"))\n"
      ],
      "metadata": {
        "id": "VwPIPfV8ZL-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è Tool Calling Setup (Function Calling)\n",
        "\n",
        "LLMs like GPT-4o-mini can go beyond just generating text ‚Äî they can **request actions** using tools you define.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What is Tool Calling?\n",
        "Instead of responding with plain text, the LLM can say:\n",
        "> ‚ÄúI want to call a function named `XXXXX` with this input.‚Äù\n",
        "\n",
        "Your app will:\n",
        "1. Detect this tool call.\n",
        "2. Run the real function (like a Python script or API call).\n",
        "3. Pass the result **back to the LLM** to continue the conversation.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß In This Example:\n",
        "We define a **language transcribtion** named `gpt-to-egytpian` with:\n",
        "- A `name`: what the LLM calls.\n",
        "- A `description`: tells the LLM what it does.\n",
        "- `parameters`: what input the tool needs (`query` in this case).\n",
        "\n",
        "This tool simulates an API that searches for trending topics.\n",
        "### üì® Tool Message Type\n",
        "When the LLM wants to invoke a tool, it responds with a special **`tool` message type** rather than plain text. This message includes the function name and a structured JSON input.  \n",
        "Your system detects this message and triggers the corresponding tool before resuming the LLM conversation with the result.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rso-BuUxZDzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# üõ†Ô∏è Tool definition\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"gpt_to_egyptian\",\n",
        "            \"description\": \"Translate English text into Egyptian‚ÄëArabic \",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"text\": {\"type\": \"string\", \"description\": \"English text\"}\n",
        "                },\n",
        "                \"required\": [\"text\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def gpt_to_egyptian(text: str) -> str:\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Translate to friendly Egyptian Arabic in a sarcastic manner.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "    return r.choices[0].message.content.strip()\n",
        "\n",
        "queries = [\n",
        "    \"Please translate to Arabic: 'Good morning, everyone!'\",\n",
        "    \"What is the Pythagorean theorem?\",\n",
        "    \"Translate to Arabic: 'I love programming in Python.'\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(f\"\\nüü† USER QUERY: {q}\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are an assistant Tutor . \"\n",
        "                \"If the user asks to translate English into Arabic, \"\n",
        "                \"call the function `gpt_to_egyptian` with the text. \"\n",
        "                \"Otherwise, answer normally.\"\n",
        "            )\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": q}\n",
        "    ]\n",
        "\n",
        "    first = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        tools=tools\n",
        "    ).choices[0]\n",
        "\n",
        "    if first.finish_reason == \"tool_calls\":\n",
        "        call = first.message.tool_calls[0]\n",
        "        args = json.loads(call.function.arguments)\n",
        "        print(\"üîß TOOL CALLED with args:\", args)\n",
        "\n",
        "        # Perform translation\n",
        "        arabic_text = gpt_to_egyptian(args[\"text\"])\n",
        "        print(\"üåê Translation returned:\", arabic_text)\n",
        "\n",
        "        # Send tool message back to GPT\n",
        "        tool_msg = {\n",
        "            \"role\": \"tool\",\n",
        "            \"tool_call_id\": call.id,\n",
        "            \"name\": \"gpt_to_egyptian\",\n",
        "            \"content\": arabic_text\n",
        "        }\n",
        "\n",
        "        final = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages + [first.message, tool_msg]\n",
        "        )\n",
        "        print(\"üü¢ GPT FINAL ANSWER:\", final.choices[0].message.content)\n",
        "\n",
        "    else:\n",
        "        # No tool; GPT responds directly\n",
        "        print(\"üü¢ GPT DIRECT ANSWER:\", first.message.content)"
      ],
      "metadata": {
        "id": "u8LAW5zgZDEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt‚ÄØEngineering  \n",
        "&nbsp;&nbsp;‚Üì  \n",
        "RAG‚ÄØContext‚ÄØRetrieval  \n",
        "&nbsp;&nbsp;‚Üì  \n",
        "LLM‚ÄØPass‚ÄØ#1‚ÄØ‚Äî‚ÄØDraft‚ÄØEnglish‚ÄØAnswer  \n",
        "&nbsp;&nbsp;‚Üì  \n",
        "Tool‚ÄëCall‚ÄØDecision  \n",
        "&nbsp;&nbsp;‚Üì  \n",
        "&nbsp;&nbsp;‚îú‚îÄ‚ÄØArabic‚ÄØrequested‚ÄØ‚Üí‚ÄØTranslation‚ÄØTool‚ÄØ(`gpt_to_egyptian`)‚ÄØ‚Üí‚ÄØFinal‚ÄØArabic‚ÄØAnswer  \n",
        "&nbsp;&nbsp;‚îî‚îÄ‚ÄØOtherwise‚ÄØ‚Üí‚ÄØFinal‚ÄØEnglish‚ÄØAnswer\n"
      ],
      "metadata": {
        "id": "Z0K1xgRiKgQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------  Full application layer ----------------\n",
        "def private_university_tutor(query: str) -> str:\n",
        "  #1) RAG ( context retrieval )\n",
        "    context = \"\\n\".join(retrieve_university_books(query))\n",
        "    print(f\"\\nüîπ QUERY: {query}\")\n",
        "    print(f\"üîπ Retrieved context:\\n{context}\\n\")\n",
        "    #2) prompt engineering\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a **university‚Äêlevel astrophysics lecturer \"\n",
        "                \"Use the provided reference to craft a concise, accurate answer. \"\n",
        "                \"Think step‚Äëby‚Äëstep, stating assumptions and intermediate deductions.\"\n",
        "                \"Write relevant equations and define each variable.\"\n",
        "                \"Do **not** hide reasoning; show the logical flow that leads to the answer\"\n",
        "                \"If the student explicitly requests Arabic output\"\n",
        "                \"(e.g. 'in Arabic', 'ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä', 'translate to Arabic'), \"\n",
        "                \"call the function `gpt_to_egyptian` with your English answer. \"\n",
        "                \"Otherwise, answer in English.\"\n",
        "            ),\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": f\"Reference book:\\n{context}\"},\n",
        "        {\"role\": \"user\",  \"content\": query},\n",
        "    ]\n",
        "\n",
        "    first = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "    ).choices[0]\n",
        "\n",
        "    print(\"üîπ finish_reason:\", first.finish_reason)\n",
        "    print(\"üîπ First assistant message:\\n\", first.message.content, \"\\n\")\n",
        "#3) tool calling\n",
        "    if first.finish_reason == \"tool_calls\":\n",
        "        call = first.message.tool_calls[0]\n",
        "        args = json.loads(call.function.arguments)\n",
        "        print(\"üîß Tool call detected:\", call.function.name, \"with\", args)\n",
        "\n",
        "        arabic = gpt_to_egyptian(args[\"text\"])\n",
        "        print(\"üåê Local translation result:\\n\", arabic, \"\\n\")\n",
        "\n",
        "        print(\"üü¢ Final assistant answer (Arabic):\\n\", arabic, \"\\n\")\n",
        "        return arabic\n",
        "\n",
        "    print(\"üü¢ Final assistant answer (no tool needed):\\n\", first.message.content, \"\\n\")\n",
        "    return first.message.content\n",
        "\n",
        "\n",
        "private_university_tutor(\"What causes gravity?\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70 + \"\\n\")\n",
        "\n",
        "private_university_tutor(\"Explain Newton's law of gravity in Arabic, please.\")\n"
      ],
      "metadata": {
        "id": "ZrjZdt-sKgkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}